---
title: "Feature Selection"
author: "Matthew Baumer"
date: "December 16, 2015"
output: html_document
---
## Download data, load packages, load data into memory, add column names
```{r}
library(caret)

# Download adult income data
url.train <- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
url.names <- "http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names"
download.file(url.train, destfile = "HousingValues.csv")
download.file(url.names, destfile = "HousingFeatureNames.txt")

# Read the training and test data into memory
train <- read.table("HousingValues.csv")
# Feature names can be found by referring to the downloaded names .txt file above
names(train) <- c("CRIM",      #per capita crime rate by town
                  "ZN",        #proportion of residential land zoned for lots over 25,000 sq.ft.
                  "INDUS",     #proportion of non-retail business acres per town
                  "CHAS",      #Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
                  "NOX",       #nitric oxides concentration (parts per 10 million)
                  "RM",        #average number of rooms per dwelling
                  "AGE",       #proportion of owner-occupied units built prior to 1940
                  "DIS",       #weighted distances to five Boston employment centres
                  "RAD",       #index of accessibility to radial highways
                  "TAX",       #full-value property-tax rate per $10,000
                  "PTRATIO",   #pupil-teacher ratio by town
                  "AA",        #1000(AA - 0.63)^2 where AA is the proportion of African Americans by town
                  "LSTAT",     #% lower status of the population
                  "MEDV")      #Median value of owner-occupied homes in $1000's
```
## How to select features

Let's start nice and easy with a no-frills OLS linear model:
```{r}
model.lm <- train(MEDV ~ .,
                  data = train,
                  method = "lm")
print(model.lm$finalModel)
```

Compare results to what you get from using lm():

```{r}
model <- lm(MEDV ~ .,
            data = train)
print(model)
```

One thing that is worth consideration is correlated predictors. Including multiple highly correlated features does not offer very much explanatory power beyond the power of including just one, but it negatively impacts variance of estimates so let's find highly correlated predictors as our first pass:

```{r}
correlations <- cor(train[,1:13])
print(correlations)
highCorrelations <- findCorrelation(correlations, cutoff = .75, verbose = TRUE)
print(highCorrelations)
```

Carefully read the output from the findCorrelation() function call; it wants to eliminate columns 3, 5, and 10. But it wants to eliminate 3 because it's correlated with 5 and 5 because it's correlated with 8. But it we remove column 5, 3 won't be strongly correlated with anything anymore! It is important to watch what your code is doing, in this case it doesn't look like 3 is necessary to remove.
```{r}
highCorrelations <- highCorrelations[-1]
print(highCorrelations)
```

Another feature selection method is available in the caret package which determines the importance of different features using an ROC curve. This function is called using varImp() on a caret model:

```{r}
plot(varImp(model.lm))
```

What happens when we remove the highly correlated variables (5 and 10)?

```{r}
train <- train[,-highCorrelations]
model.lm <- train(MEDV ~ .,
                  data = train,
                  method = "lm")
plot(varImp(model.lm))
```

The caret package has another nice prepackaged methodology for feature selection: rfeControl() which uses a method called Recursive Feature Elimination. This code works as follows: 

* Define the control using a random forest selection function
* Run the RFE algorithm
* Summarize and plot the results

```{r}
set.seed(134)
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
results <- rfe(train[,1:11], train[,12], sizes=c(1:11), rfeControl=control)
print(results)
predictors(results)
plot(results, type=c("g", "o"))
```

Lowest RMSE at 9 predictors but this is only slightly better than what was attained with 6, so might still be better to only include 6 for final model. The 9 that are suggested are listed with predictors(results).

Note that the top 5 features from the RFE methodology isn't quite the same as from using varImp()! The most appropriate selection of features will ultimately be a design decision you have to make based on which seems to be the best set for prediction. These methodologies are simply ways for you to make that decision in an informed way but at the end of the day it's up to YOU to make sure that your selection is the best it can be.

# Lasso for feature selection

The regularized regression technique called the "lasso" essentially is an OLS regression with a constraint applied to the betas, namely that the sum of the absolute values of all of the betas are less than some parameter. This will be discussed in greater detail in another example.

```{r}
train <- read.table("HousingValues.csv")

names(train) <- c("CRIM",      #per capita crime rate by town
                  "ZN",        #proportion of residential land zoned for lots over 25,000 sq.ft.
                  "INDUS",     #proportion of non-retail business acres per town
                  "CHAS",      #Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
                  "NOX",       #nitric oxides concentration (parts per 10 million)
                  "RM",        #average number of rooms per dwelling
                  "AGE",       #proportion of owner-occupied units built prior to 1940
                  "DIS",       #weighted distances to five Boston employment centres
                  "RAD",       #index of accessibility to radial highways
                  "TAX",       #full-value property-tax rate per $10,000
                  "PTRATIO",   #pupil-teacher ratio by town
                  "AA",        #1000(AA - 0.63)^2 where AA is the proportion of African Americans by town
                  "LSTAT",     #% lower status of the population
                  "MEDV")      #Median value of owner-occupied homes in $1000's
train <- train[,-9]
model.lasso <- train(MEDV ~ .,
                     data = train,
                     method = "lasso")
print(model.lasso$finalModel)
plot(model.lasso$finalModel, xvar = "penalty")
```

This plot shows which variables get set to zero in the lasso as lambda (the penalty parameter) is increased (forgive the missing labels! But the important ones are still visible). You can also see the order of importance of variables by looking at "Step" in the final model printout above the plot.

Which variables get set to zero last using the lasso approach (i.e. are the most important)? How do these compare to what we found using the first two methods?
